{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "317a2684",
   "metadata": {},
   "source": [
    "######## q) if in a pair plot the clusters between detergent and grocery are showing nicely, what does this indicated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e755d9",
   "metadata": {},
   "source": [
    "If the clusters between detergent and grocery are showing nicely, it means that the values of these two variables tend to be similar for some subset of the data points. This may suggest that customers who buy more detergent also tend to buy more groceries, or vice versa. A strong positive correlation between these two variables means that as one variable increases, the other variable also tends to increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a0cb94",
   "metadata": {},
   "source": [
    "#### diff between PCA And LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e2939",
   "metadata": {},
   "source": [
    "CA and LDA are both techniques used for dimensionality reduction, but they have different goals and applications. PCA is an unsupervised technique used to reduce the dimensionality of the data by finding a new set of variables that explain the maximum variance in the original data, while LDA is a supervised technique used to reduce the dimensionality of the data while preserving the class separability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a2042",
   "metadata": {},
   "source": [
    "#### differences between LLE and LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6300a12",
   "metadata": {},
   "source": [
    "Here are some key differences between LLE and LDA:\n",
    "\n",
    "Goal: LLE aims to preserve the local structure of the data, while LDA aims to maximize the separation between different classes.\n",
    "Linearity: LLE is a non-linear technique, while LDA is a linear technique.\n",
    "Supervision: LLE is an unsupervised technique, while LDA is a supervised technique that requires the class labels of the data.\n",
    "Input: LLE takes the entire dataset as input, while LDA takes both the features and the class labels as input.\n",
    "Output: LLE produces a lower-dimensional representation of the data that preserves the local structure, while LDA produces new variables (linear discriminants) that are linear combinations of the original variables and are optimized for class separation.\n",
    "Application: LLE is commonly used for manifold learning and visualization, while LDA is commonly used for classification tasks.\n",
    "In summary, LLE and LDA are both techniques used for dimensionality reduction, but they have different goals and applications. LLE is a non-linear technique used to preserve the local structure of the data, while LDA is a linear technique used to maximize the separation between different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b219f4d",
   "metadata": {},
   "source": [
    "TSNE- t-SNE (t-Distributed Stochastic Neighbor Embedding) is a non-linear technique used for data visualization and dimensionality reduction, particularly effective in visualizing high-dimensional data in a low-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f13da",
   "metadata": {},
   "source": [
    "1)Hierarchical vs GMM Clustering\n",
    "\n",
    "Difference between hierarchical and gmm clustering\n",
    "\n",
    "Hierarchical clustering and Gaussian Mixture Model (GMM) clustering are both unsupervised machine learning techniques used for clustering similar data points.\n",
    "\n",
    "Hierarchical clustering is a bottom-up approach that builds a hierarchy of clusters by iteratively merging smaller clusters into larger ones based on a similarity criterion. The process continues until all the data points belong to a single cluster or a predetermined number of clusters is reached. The result of hierarchical clustering is a dendrogram, which represents the hierarchy of clusters.\n",
    "\n",
    "On the other hand, GMM clustering is a probabilistic model that assumes that the data is generated from a mixture of Gaussian distributions. The goal of GMM clustering is to estimate the parameters of these Gaussian distributions and assign each data point to the most likely distribution. GMM clustering can handle non-spherical clusters and can estimate the number of clusters automatically.\n",
    "\n",
    "Here are some key differences between hierarchical and GMM clustering:\n",
    "\n",
    "Hierarchical clustering produces a tree-like structure (dendrogram) that represents the hierarchy of clusters, while GMM clustering assigns each data point to a specific cluster.\n",
    "\n",
    "Hierarchical clustering requires a predefined linkage method (e.g., single linkage, complete linkage, or average linkage) to determine the distance between clusters, while GMM clustering uses a likelihood-based approach to estimate the parameters of the Gaussian distributions.\n",
    "\n",
    "Hierarchical clustering is sensitive to noise and outliers since it builds a hierarchy of clusters based on similarity, while GMM clustering can handle noise and outliers by assigning them to the closest Gaussian distribution.\n",
    "\n",
    "Hierarchical clustering is more computationally intensive than GMM clustering, especially for large datasets, since it requires pairwise distance calculations between all data points.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a6f4bf",
   "metadata": {},
   "source": [
    "2) \n",
    "Steps of gmm clustering \n",
    "\n",
    "The steps involved in Gaussian Mixture Model (GMM) clustering are:\n",
    "\n",
    "Initialization: Initialize the parameters of the Gaussian mixture model, including the number of clusters, mean, covariance, and mixing coefficients.\n",
    "\n",
    "Expectation step: Calculate the probability of each data point belonging to each Gaussian component using Bayes' theorem. This step computes the posterior probability of each data point given the current model parameters.\n",
    "\n",
    "Maximization step: Update the model parameters to maximize the log-likelihood of the data given the posterior probabilities obtained from the previous step. This step involves updating the mean, covariance, and mixing coefficients of each Gaussian component based on the current estimates of the posterior probabilities.\n",
    "\n",
    "Convergence check: Check if the log-likelihood of the data has converged or if the model parameters have reached a stable state. If not, repeat steps 2 and 3 until convergence.\n",
    "\n",
    "Model selection: Determine the optimal number of clusters that best fits the data using a criterion such as the Bayesian Information Criterion (BIC) or the Akaike Information Criterion (AIC).\n",
    "\n",
    "Cluster assignment: Assign each data point to the cluster with the highest probability based on the final estimates of the model parameters.\n",
    "\n",
    "Visualization and interpretation: Visualize the results and interpret the clusters to gain insights into the underlying patterns and structures in the data.\n",
    "\n",
    "Overall, GMM clustering is an iterative algorithm that alternates between computing the posterior probabilities and updating the model parameters until convergence. The algorithm is sensitive to the initial values of the model parameters, and therefore, multiple initializations with different random seeds are often used to ensure that the algorithm converges to the global optimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c553d5d",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "It seems like you are performing Principal Component Analysis (PCA) on the wine dataset. PCA is a popular technique used to reduce the dimensionality of a dataset by projecting it onto a lower-dimensional space while preserving most of the variation in the original data.\n",
    "\n",
    "In your code, you first split the data into predictor variables (X) and target variable (y). Then you standardize the predictor variables using the StandardScaler() function, which transforms the variables to have a mean of 0 and a standard deviation of 1. This step is important because PCA is sensitive to the scale of the variables.\n",
    "\n",
    "Next, you compute the covariance matrix of the standardized data using np.cov(X_scaled.T). The covariance matrix shows how each variable in the dataset is related to each other variable. The diagonal elements of the covariance matrix show the variances of each variable, while the off-diagonal elements show the covariances between variables.\n",
    "\n",
    "You then find the eigenvalues and eigenvectors of the covariance matrix using np.linalg.eig(cm). Eigenvalues represent the amount of variance explained by each principal component, while eigenvectors represent the directions in which the data vary the most.\n",
    "\n",
    "You sort the eigenvalues in descending order and calculate the explained variance and cumulative explained variance for each principal component. The explained variance tells you how much variance each principal component explains relative to the total variance in the dataset. The cumulative explained variance shows how much of the total variance in the dataset is explained by each successive principal component.\n",
    "\n",
    "You then plot the explained variance and cumulative explained variance for each principal component to decide on the number of principal components to use in your analysis. In your case, you chose to use only the first two principal components.\n",
    "\n",
    "Finally, you construct the projection matrix by stacking the first two eigenvectors horizontally and multiplying the standardized data by the projection matrix to obtain the two-dimensional representation of the data. You then visualize the projected data using scatterplots, with different markers representing the three different wine classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d78028d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#CODE- PCA\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mwine\u001b[49m\n\u001b[0;32m      6\u001b[0m y\u001b[38;5;241m=\u001b[39mwine[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWine\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m X\u001b[38;5;241m=\u001b[39mwine\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWine\u001b[39m\u001b[38;5;124m'\u001b[39m],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wine' is not defined"
     ]
    }
   ],
   "source": [
    "#CODE- PCA\n",
    "\n",
    "\n",
    "wine\n",
    "\n",
    "y=wine['Wine']\n",
    "X=wine.drop(['Wine'],axis=1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_scaled=sc.fit_transform(X)\n",
    "X_scaled\n",
    "\n",
    "#Construction of covariance marix\n",
    "cm=np.cov(X_scaled.T)\n",
    "cm\n",
    "\n",
    "#Finding eigen value, eigen vector\n",
    "eig_val,eig_vec=np.linalg.eig(cm)\n",
    "\n",
    "#Sorting eigen values\n",
    "\n",
    "sorted_eig_val=[i for i in sorted(eig_val, reverse=True)]\n",
    "sorted_eig_val\n",
    "\n",
    "#Choosing the dimension =2\n",
    "\n",
    "tot=sum(sorted_eig_val)\n",
    "tot\n",
    "\n",
    "exp_var=[(i/tot) for i in sorted_eig_val]\n",
    "exp_var\n",
    "\n",
    "cum_exp_var=np.cumsum(exp_var)\n",
    "cum_exp_var\n",
    "\n",
    "#Plotting\n",
    "plt.bar(range(1,14), exp_var,label='Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel(' Explained Variance')\n",
    "plt.legend();\n",
    "\n",
    "#Construction of projection matrix\n",
    "eigen_pair=[(np.abs(eig_val[i]),eig_vec[:,i]) for i in range(len(eig_val))]\n",
    "eigen_pair\n",
    "\n",
    "# Taking only 2 dimension\n",
    "\n",
    "w=np.hstack((eigen_pair[0][1][:,np.newaxis],\n",
    "            eigen_pair[1][1][:,np.newaxis]))\n",
    "\n",
    "#Transforming 13 dim data to 2 dim\n",
    "\n",
    "X_scaled.shape\n",
    "new_X=X_scaled.dot(w)\n",
    "\n",
    "#Visualising the projected data\n",
    "for l in np.unique(y):\n",
    "    plt.scatter(new_X[y==1,0], new_X[y==1,1],marker='s')\n",
    "    plt.scatter(new_X[y==2,0], new_X[y==2,1],marker='x')\n",
    "    plt.scatter(new_X[y==3,0], new_X[y==3,1],marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afb9d06",
   "metadata": {},
   "source": [
    "# AIC-BIC\n",
    "AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) are two commonly used measures for selecting the optimal number of components in a Gaussian Mixture Model (GMM).\n",
    "\n",
    "AIC and BIC are based on different criteria for model selection, but they both penalize models with a larger number of parameters to prevent overfitting. In general, the lower the AIC or BIC score, the better the model.\n",
    "\n",
    "When selecting the optimal number of components in a GMM, one can fit models with different numbers of components and compute their AIC and BIC scores. The model with the lowest AIC or BIC score is considered the best fit for the data.\n",
    "\n",
    "It is important to note that both AIC and BIC are based on assumptions about the distribution of the data and the GMM model. Therefore, it is recommended to use these measures along with other model selection techniques and to validate the results using different data sets or techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GMM\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "n_comps=np.arange(1,20,1)\n",
    "aic_score=[]\n",
    "bic_score=[]\n",
    "for n in n_comps:\n",
    "    model=GaussianMixture(n_components=n,\n",
    "                          random_state=10,\n",
    "                        n_init=5)\n",
    "    model.fit(X)\n",
    "    aic_score.append(model.aic(X))\n",
    "    bic_score.append(model.bic(X))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a94570",
   "metadata": {},
   "source": [
    "explain code in detail\n",
    "\n",
    "\n",
    "This code imports the GaussianMixture class from the scikit-learn library and defines a range of values for the number of components in the Gaussian mixture model (GMM) using the np.arange() function. It then creates empty lists to store the AIC and BIC scores for each value of the number of components.\n",
    "\n",
    "The code then creates a for loop that iterates through each value of n_comps. Inside the loop, it creates a GaussianMixture object with the current value of n as the number of components, a fixed random state of 10 for reproducibility, and 5 initializations to prevent the algorithm from getting stuck in local optima.\n",
    "\n",
    "The model object is then fit to the data X using the fit() method. The AIC and BIC scores for the model are then computed using the aic() and bic() methods, respectively, and added to the aic_score and bic_score lists.\n",
    "\n",
    "Finally, the loop ends and the aic_score and bic_score lists contain the AIC and BIC scores for each value of n_comps. These scores can be used to compare the performance of different models with different numbers of components and select the optimal number of components for the GMM.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
